{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary modules\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "\n",
    "# Downloading required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "next_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up stopwords and lemmatizer\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_docs_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading documents from folder\n",
    "def load_documents(folder_path):\n",
    "    data = {}\n",
    "    doc_id_to_filename = {}\n",
    "    doc_id = 0\n",
    "\n",
    "    print(f'Scanning folder: {folder_path}')\n",
    "    for filename in os.listdir(folder_path):\n",
    "        print(f'Found file: {filename}')\n",
    "        if filename.endswith(('.txt', '.doc', '.docx', '.pdf')):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            content = ''\n",
    "            \n",
    "            if filename.endswith('.txt'):\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "            \n",
    "            elif filename.endswith(('.doc', '.docx')):\n",
    "                from docx import Document\n",
    "                doc = Document(filepath)\n",
    "                content = '\\n'.join([para.text for para in doc.paragraphs])\n",
    "            \n",
    "            elif filename.endswith('.pdf'):\n",
    "                import PyPDF2\n",
    "                with open(filepath, 'rb') as pdf_file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "                    for page in pdf_reader.pages:\n",
    "                        content += page.extract_text()\n",
    "                \n",
    "            if content:\n",
    "                data[doc_id] = content\n",
    "                doc_id_to_filename[doc_id] = filename\n",
    "                print(f'Loaded doc_id {doc_id} -> {filename}')\n",
    "                doc_id += 1\n",
    "\n",
    "    print(f'Total documents loaded: {len(data)}')\n",
    "    return data, doc_id_to_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "clean_text_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text by removing unwanted characters and lemmatizing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS and len(word) > 1]\n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vectorize_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vector space model using TF-IDF\n",
    "def build_vector_space_model(data):\n",
    "    cleaned_docs = [clean_text(content) for content in data.values()]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(cleaned_docs)\n",
    "    return tfidf_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "similarity_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating similarity scores between documents\n",
    "def calculate_similarity(tfidf_matrix, doc_id_to_filename):\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    num_docs = len(doc_id_to_filename)\n",
    "    print('Similarity scores between documents: ')\n",
    "    for i in range(num_docs):\n",
    "        for j in range(i + 1, num_docs):\n",
    "            score = similarity_matrix[i][j]\n",
    "            doc1 = doc_id_to_filename[i]\n",
    "            doc2 = doc_id_to_filename[j]\n",
    "            print(f'Similarity between {doc1} and {doc2}: {score:.4f}')\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning folder: ./documents\n",
      "Found file: artificial_intelligence.txt\n",
      "Loaded doc_id 0 -> artificial_intelligence.txt\n",
      "Found file: deep_learning.txt\n",
      "Loaded doc_id 1 -> deep_learning.txt\n",
      "Found file: information_retrieval.txt\n",
      "Loaded doc_id 2 -> information_retrieval.txt\n",
      "Found file: machine_learning.txt\n",
      "Loaded doc_id 3 -> machine_learning.txt\n",
      "Found file: nlp.txt\n",
      "Loaded doc_id 4 -> nlp.txt\n",
      "Found file: text_mining.txt\n",
      "Loaded doc_id 5 -> text_mining.txt\n",
      "Found file: vector_space_model.txt\n",
      "Loaded doc_id 6 -> vector_space_model.txt\n",
      "Total documents loaded: 7\n",
      "Doc 0 cleaned text (first 100 chars): artificial intelligence transforming future artificial intelligence ai become one transformative tec...\n",
      "Doc 1 cleaned text (first 100 chars): deep learning neural network scale deep learning specialized subset machine learning us artificial n...\n",
      "Doc 2 cleaned text (first 100 chars): information retrieval finding relevant document information retrieval ir process obtaining informati...\n",
      "Doc 3 cleaned text (first 100 chars): machine learning learning data machine learning ml subset artificial intelligence focus developing a...\n",
      "Doc 4 cleaned text (first 100 chars): natural language processing understanding human language natural language processing nlp branch arti...\n",
      "Doc 5 cleaned text (first 100 chars): text mining extracting knowledge text text mining also known text analytics process extracting meani...\n",
      "Doc 6 cleaned text (first 100 chars): vector space model document similarity vector space model vsm fundamental technique information retr...\n",
      "Similarity scores between documents: \n",
      "Similarity between artificial_intelligence.txt and deep_learning.txt: 0.1920\n",
      "Similarity between artificial_intelligence.txt and information_retrieval.txt: 0.0719\n",
      "Similarity between artificial_intelligence.txt and machine_learning.txt: 0.1610\n",
      "Similarity between artificial_intelligence.txt and nlp.txt: 0.1550\n",
      "Similarity between artificial_intelligence.txt and text_mining.txt: 0.0540\n",
      "Similarity between artificial_intelligence.txt and vector_space_model.txt: 0.0305\n",
      "Similarity between deep_learning.txt and information_retrieval.txt: 0.1336\n",
      "Similarity between deep_learning.txt and machine_learning.txt: 0.2559\n",
      "Similarity between deep_learning.txt and nlp.txt: 0.1716\n",
      "Similarity between deep_learning.txt and text_mining.txt: 0.0645\n",
      "Similarity between deep_learning.txt and vector_space_model.txt: 0.0850\n",
      "Similarity between information_retrieval.txt and machine_learning.txt: 0.1336\n",
      "Similarity between information_retrieval.txt and nlp.txt: 0.1589\n",
      "Similarity between information_retrieval.txt and text_mining.txt: 0.0895\n",
      "Similarity between information_retrieval.txt and vector_space_model.txt: 0.4848\n",
      "Similarity between machine_learning.txt and nlp.txt: 0.1165\n",
      "Similarity between machine_learning.txt and text_mining.txt: 0.0817\n",
      "Similarity between machine_learning.txt and vector_space_model.txt: 0.0893\n",
      "Similarity between nlp.txt and text_mining.txt: 0.0710\n",
      "Similarity between nlp.txt and vector_space_model.txt: 0.0786\n",
      "Similarity between text_mining.txt and vector_space_model.txt: 0.1013\n",
      "Results saved to similarity_results.txt\n",
      "For the report, include snapshots of this output and code cells.\n",
      "Publish this notebook on GitHub and provide the link.\n"
     ]
    }
   ],
   "source": [
    "# Running the main function\n",
    "def main():\n",
    "    folder_path = './documents'  # Documents folder path relative to notebook location\n",
    "    data, doc_id_to_filename = load_documents(folder_path)\n",
    "    \n",
    "    # Printing sample cleaned text for verification\n",
    "    for doc_id, content in data.items():\n",
    "        cleaned = clean_text(content)\n",
    "        print(f'Doc {doc_id} cleaned text (first 100 chars): {cleaned[:100]}...')\n",
    "    \n",
    "    tfidf_matrix, vectorizer = build_vector_space_model(data)\n",
    "    similarity_matrix = calculate_similarity(tfidf_matrix, doc_id_to_filename)\n",
    "    \n",
    "    # Saving results to a file for report\n",
    "    with open('similarity_results.txt', 'w', encoding='utf-8') as result_file:\n",
    "        result_file.write('Similarity scores:\\n')\n",
    "        num_docs = len(doc_id_to_filename)\n",
    "        for i in range(num_docs):\n",
    "            for j in range(i + 1, num_docs):\n",
    "                score = similarity_matrix[i][j]\n",
    "                doc1 = doc_id_to_filename[i]\n",
    "                doc2 = doc_id_to_filename[j]\n",
    "                result_file.write(f'{doc1} and {doc2}: {score:.4f}\\n')\n",
    "    print('Results saved to similarity_results.txt')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
